{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "list_titles,list_categories,list_stars,list_prices,is_in_stock = [],[],[],[],[]\n",
    "\n",
    "def get_books(book_containers,html_soup):\n",
    "\n",
    "    for j in range(0,len(book_containers)):\n",
    "        list_prices.append(book_containers[j].find_all('div')[1].find_all('p')[0].text.split('Â£')[1])\n",
    "                \n",
    "        is_in_stock.append( 'Yes' if 'In stock' in book_containers[j].find_all('div')[1].find_all('p')[1].text else 'No')\n",
    "                \n",
    "        list_stars.append(book_containers[j].find_all('p')[0]['class'][1])\n",
    "                \n",
    "        list_titles.append(book_containers[j].find_all('div')[0].a.img['alt'])\n",
    "                \n",
    "        list_categories.append(html_soup.title.text.split('|')[0].strip())\n",
    "      \n",
    "    return\n",
    "\n",
    "###-  Set the correct path where chrome webdriver is located\n",
    "os.environ[\"WDPATH\"] = \"C:\\\\Users\\pedro\\Dropbox\\pythonApplications\\DataEngineering_project\"\n",
    "wdpath = os.environ[\"WDPATH\"]\n",
    "os.chdir(\"./subroutines\")\n",
    "from getCategories import get_categories\n",
    "from checkWebdriver import check_webdriver\n",
    "os.chdir('../')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Checking if webdriver's path is OK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chrome Web Driver location is OK\n"
     ]
    }
   ],
   "source": [
    "if check_webdriver(wdpath):\n",
    "    pass\n",
    "else:\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Getting all categories__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pages not updated\n",
      "\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "urls_to_scrape = []\n",
    "if datetime.datetime.today().weekday()==0:\n",
    "    try:\n",
    "        urls_to_scrape = get_categories('http://books.toscrape.com/',wdpath)\n",
    "    except RemoteDisconnected:\n",
    "        print('\\nNo internet connection!\\n')\n",
    "        exit()\n",
    "    if urls_to_scrape != []:\n",
    "        print('All categories escraped.')\n",
    "        outFile = open('all_pages.txt','w')\n",
    "        if outFile.mode == 'w':\n",
    "            for link in urls_to_scrape:\n",
    "                outFile.write(link+'\\n')\n",
    "            outFile.close()\n",
    "        else:\n",
    "            print('\\nFile is not in writing mode\\n')\n",
    "            exit()\n",
    "    else:\n",
    "        print('Something went wrong when searching for categories.\\n')\n",
    "        exit()\n",
    "else:\n",
    "    print('\\nPages not updated\\n')\n",
    "    inFile = open('all_pages.txt','r')\n",
    "    if inFile.mode == 'r':\n",
    "        urls_to_scrape = inFile.readlines()\n",
    "    else:\n",
    "        print('\\nFile is not in reading mode\\n')\n",
    "        exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Getting data from each page__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import grequests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "reqs = (grequests.get(link.strip()) for link in urls_to_scrape)\n",
    "resp=grequests.imap(reqs, grequests.Pool(10))\n",
    "for r in resp:\n",
    "    html_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    book_containers = html_soup.find_all('li', class_ = 'col-xs-6 col-sm-4 col-md-3 col-lg-3')\n",
    "    get_books(book_containers,html_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating a DataFrame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list_titles+list_categories+list_stars+list_prices+is_in_stock\n",
    "\n",
    "if res ==[]:\n",
    "    print('No data scraped.\\n')\n",
    "    exit()\n",
    "else:\n",
    "    df = pd.DataFrame({'Title':list_titles,'Category':list_categories,'Stars':list_stars,'Price':list_prices,'In Stock':is_in_stock})\n",
    "\n",
    "    df['Stars'] = df['Stars'].map({'One':1,'Two':2,'Three':3,'Four':4,'Five':5})\n",
    "    df['Title'] = df['Title'].apply(lambda x: x.replace('\\x80\\x99',\"'\") if '\\x80\\x99' in x else x)\n",
    "    df['Title'] = df['Title'].apply(lambda x: x.replace('\"',\"'\"))\n",
    "\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Checking if formatting is OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking if it's full of NaN\n",
    "if df.isnull().sum().sum() == df.shape[0]*df.shape[1]:\n",
    "    print('Only NaN values scraped. Something went wrong...\\n')\n",
    "    exit()\n",
    "\n",
    "## Checking stars\n",
    "nan_in_stars = df.Stars.isnull().sum()\n",
    "if nan_in_stars == len(df):\n",
    "    print('All stars scraped are NULL. Ending execution...\\n')\n",
    "    exit()\n",
    "    \n",
    "## Checking if there's something that differs from 'Yes', 'No' or 'Nan' in \"In Stock\" column\n",
    "for count in df['In Stock'].dropna().value_counts().index:\n",
    "    if df['In Stock'].isnull().sum() == len(df):\n",
    "        print('All stars scraped are NULL. Ending execution...\\n')\n",
    "        exit()\n",
    "    elif count !='Yes' and count !='No':\n",
    "        print('Invalid value in \"In Stock\" column. Exiting...')\n",
    "        exit()\n",
    "        \n",
    "## Checking if there's 'Nan' in title column\n",
    "df['Title'].fillna('NULL', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a query to insert data into 'scraped' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./sql_files/insertInto_scrapedTable.sql','w', encoding=\"utf-8\") as outFile:\n",
    "    outFile.write('USE book_club_web_data;\\n\\n')\n",
    "    outFile.write('INSERT INTO scraped(title,category,stars,price,is_in_stock) VALUES\\n')\n",
    "    for row in df.iloc:\n",
    "        if row.name == df.iloc[-1].name:\n",
    "            value = '(\"'+'\",\"'.join(row.apply(lambda x: str(x)).values)+'\");\\n'\n",
    "            value = value.replace('\"NULL\"','NULL')\n",
    "            outFile.write(value)\n",
    "        else:\n",
    "            value = '(\"'+'\",\"'.join(row.apply(lambda x: str(x)).values)+'\"),\\n'\n",
    "            value = value.replace('\"NULL\"','NULL')\n",
    "            outFile.write(value)\n",
    "    outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
